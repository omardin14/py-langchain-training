# RAG Evaluation

This module introduces **evaluation methods for RAG (Retrieval-Augmented Generation) applications**. Evaluating RAG systems is crucial for measuring performance, identifying issues, and improving the quality of your applications.

<!-- lesson:page Evaluating RAG Applications -->
## Overview

RAG architecture is made up of several processes, and there are a few places where performance can be measured:

- **Evaluate the retrieval process**: Check if the retrieved documents are relevant to the query
- **Evaluate the generation process**: See if the LLM hallucinated or misinterpreted the prompt
- **Evaluate the final output**: Measure the performance of the whole system

This module covers different evaluation approaches, from simple LLM-based output analysis to advanced RAGAS (RAG Assessment) metrics.

## Concepts Covered

- **Output Analysis**: Using LLMs to evaluate correctness
- **RAGAS Framework**: RAG Assessment for comprehensive evaluation
- **Faithfulness Metric**: Measuring if output is derived from context
- **Context Precision Metric**: Measuring relevance of retrieved documents
- **Performance Measurement**: Evaluating different components of RAG systems

## Evolving the RAG Architecture

RAG architecture consists of multiple interconnected processes. To improve and optimize RAG applications, we need to measure performance at different stages:

![RAG Evaluation Process](../utils/media/rag_evaloution.png)

### Performance Measurement Points

1. **Retrieval Process**
   - Are the retrieved documents relevant to the query?
   - Does the retriever find the right information?
   - Are important documents being missed?

2. **Generation Process**
   - Does the LLM hallucinate information?
   - Does the LLM misinterpret the prompt?
   - Is the generated answer coherent and accurate?

3. **Final Output**
   - How does the whole system perform end-to-end?
   - Is the final answer correct and useful?
   - Does it meet user expectations?

<!-- lesson:page Output Analysis -->
## Output Analysis

### Using LLMs to Evaluate Correctness

We can use LLMs to measure the correctness of the final output by comparing it to a reference answer.

**Key Components:**
- **Prompt Template**: Instructs the model to compare strings and evaluate correctness
- **Large Language Model**: Used for evaluation (temperature set to zero to minimize variability)
- **Reference Answer**: The correct answer to compare against
- **Predicted Answer**: The answer generated by the RAG system

### How It Works

1. Define a prompt template that instructs the model to compare answers
2. Use an LLM with temperature=0 for consistent evaluation
3. Compare predicted answer to reference answer
4. Return CORRECT or INCORRECT (or a score)

### Scoring

- **Score of 1 (CORRECT)**: The predicted response matches the reference answer
- **Score of 0 (INCORRECT)**: The predicted response was incorrect when compared to the reference answer

### Example

#### Step 1: Create the Evaluation Prompt

Define a prompt template that instructs the model to compare a predicted answer against a reference answer:

```python
from langchain_core.prompts import PromptTemplate

prompt_template = """You are an evaluator comparing two answers to determine if they match.

Question being evaluated:
{query}

Expected correct answer:
{answer}

Answer to evaluate:
{result}

Compare the answers and respond with only CORRECT or INCORRECT:

Grade:"""

prompt = PromptTemplate(
    input_variables=["query", "answer", "result"],
    template=prompt_template
)
```

#### Step 2: Create the Evaluation Chain

Use an LLM with `temperature=0` for consistent, deterministic evaluation results:

```python
from langchain_openai import ChatOpenAI

eval_llm = ChatOpenAI(temperature=0, model="gpt-4o-mini")
eval_chain = prompt | eval_llm
```

#### Step 3: Run the Evaluation

Invoke the chain with a query, the expected answer, and the predicted result:

```python
result = eval_chain.invoke({
    "query": "What are the two primary stages in document processing systems?",
    "answer": "Retrieval and generation",
    "result": "Loading and storage"
})
# Returns: INCORRECT (score: 0)
```

<!-- lesson:page RAGAS and Faithfulness -->
## RAGAS (RAG Assessment)

**RAGAS** was designed to evaluate both the retrieval and generation components of a RAG application. It provides specialized metrics that go beyond simple correctness checks.

### Key Features

- **Comprehensive Evaluation**: Covers both retrieval and generation
- **Specialized Metrics**: Designed specifically for RAG applications
- **Quantitative Scores**: Provides numerical scores for comparison
- **Multiple Metrics**: Different metrics for different aspects

## Faithfulness Metric

### What is Faithfulness?

**Faithfulness** assesses whether the generated output represents the retrieved documents well. It is calculated using LLMs to assess the ratio of faithful claims that can be derived from the context to the total number of claims.

### How It Works

1. Extract claims from the generated answer
2. Check if each claim can be derived from the retrieved context
3. Calculate the ratio of faithful claims to total claims
4. Return a score between 0 and 1

### Scoring

- **Score of 1.0**: Perfect faithfulness - all claims can be fully inferred from the context
- **Score of 0.0**: No faithfulness - claims cannot be derived from the context
- **Score between 0 and 1**: Proportion of claims that are faithful

### Why It Matters

Faithfulness helps identify:
- **Hallucination**: When the LLM generates information not in the context
- **Misinterpretation**: When the LLM misreads the context
- **Over-generation**: When the LLM adds unsupported details

### Example

#### Step 1: Create the LLM and Embeddings

Set up the language model and embeddings that RAGAS will use for evaluation:

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

llm = ChatOpenAI(model="gpt-4o-mini")
embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key="...")
```

#### Step 2: Create the Faithfulness Evaluator

Create an `EvaluatorChain` configured with the faithfulness metric:

```python
from ragas.integrations.langchain import EvaluatorChain
from ragas.metrics import faithfulness

faithfulness_chain = EvaluatorChain(
    metric=faithfulness,
    llm=llm,
    embeddings=embeddings
)
```

#### Step 3: Run the Evaluation

Pass the question, generated answer, and retrieved contexts. RAGAS checks if each claim in the answer can be derived from the contexts:

```python
eval_result = faithfulness_chain({
    "question": "How do information systems combine document search with text generation?",
    "answer": "Information systems combine document search with text generation by first finding relevant documents from a knowledge base, then using those documents to generate accurate responses.",
    "contexts": [
        "Information systems integrate document search with text generation by first retrieving relevant passages from a knowledge base, then using those passages to inform the text generation process.",
        "By incorporating search mechanisms, information systems leverage external knowledge sources, allowing the system to access current information."
    ]
})

print(eval_result['faithfulness'])  # Output: 1.0 (perfect faithfulness)
```

<!-- lesson:page Context Precision and Best Practices -->
## Context Precision Metric

### What is Context Precision?

**Context precision** measures how relevant the retrieved documents are to the query. A context precision score closer to one means the retrieved context is highly relevant.

### How It Works

1. Evaluate each retrieved document's relevance to the query
2. Consider the order of retrieved documents (earlier = more important)
3. Calculate precision based on relevant documents
4. Return a score between 0 and 1

### Scoring

- **Score of 1.0**: All retrieved contexts are highly relevant
- **Score of 0.0**: Retrieved contexts are irrelevant
- **Score between 0 and 1**: Proportion of relevant contexts

### Why It Matters

Context precision helps identify:
- **Retrieval Quality**: Whether the retriever finds relevant documents
- **Ranking Issues**: Whether relevant documents are ranked correctly
- **Query Understanding**: Whether the query is being interpreted correctly

### Example

```python
from ragas.metrics import context_precision

context_precision_chain = EvaluatorChain(
    metric=context_precision,
    llm=llm,
    embeddings=embeddings
)

eval_result = context_precision_chain({
    "question": "How do information systems combine document search with text generation?",
    "ground_truth": "Information systems combine document search with text generation by retrieving relevant documents and using them to generate accurate responses.",
    "contexts": [
        "Information systems integrate document search with text generation by first retrieving relevant passages from a knowledge base.",
        "By incorporating search mechanisms, information systems leverage external knowledge sources, allowing access to current information.",
        "Database systems store and organize information for efficient retrieval."  # Less relevant
    ]
})

print(f"Context Precision: {eval_result['context_precision']}")
# Output: Context Precision: 0.67 (2 out of 3 contexts are highly relevant)
```

## When to Use Each Evaluation Method

### Use Output Analysis When:
- You have reference answers
- You want simple correctness checking
- You need quick evaluation
- You're evaluating final outputs only

### Use Faithfulness When:
- You want to detect hallucination
- You need to verify answer is derived from context
- You're evaluating generation quality
- You want to measure claim accuracy

### Use Context Precision When:
- You want to evaluate retrieval quality
- You need to measure document relevance
- You're optimizing the retriever
- You want to check ranking quality

## Best Practices

1. **Set Temperature to Zero**
   - Use `temperature=0` for evaluation LLMs
   - Minimizes variability in evaluation results
   - Ensures consistent scoring

2. **Use Multiple Metrics**
   - Don't rely on a single metric
   - Combine faithfulness and context precision
   - Evaluate both retrieval and generation

3. **Create Good Reference Answers**
   - Use high-quality ground truth
   - Ensure reference answers are accurate
   - Consider multiple valid answers

4. **Evaluate Regularly**
   - Set up evaluation pipelines
   - Monitor metrics over time
   - Track improvements and regressions

## Summary

This module introduced:
- **Output Analysis**: Using LLMs to evaluate correctness
- **RAGAS Framework**: Comprehensive RAG evaluation
- **Faithfulness Metric**: Measuring if output is derived from context
- **Context Precision Metric**: Measuring relevance of retrieved documents
- **Performance Measurement**: Evaluating different RAG components

Evaluating RAG applications is essential for building high-quality systems. These tools help you measure, understand, and improve your RAG applications at every stage.

<!-- lesson:end -->

## Prerequisites

This module builds on concepts from:
- **011-lcel-retrival-chain**: Understanding RAG chains
- **014-advanced-retrieval**: Understanding retrieval methods

### Setting Up Your Environment

**Complete Setup Steps:**

1. **Create the `.env` file** using the Makefile:
   ```bash
   make setup
   ```
   This creates a `.env` file from `.env.example` (or creates a template if it doesn't exist).

2. **Edit the `.env` file** and add your OpenAI API key:
   ```
   OPENAI_API_KEY=your-actual-api-key-here
   ```
   > **Note:** This module requires an OpenAI API key for evaluation. Get your key from: https://platform.openai.com/api-keys

3. **Set up virtual environment and install dependencies:**
   ```bash
   make install
   ```
   This creates a Python virtual environment and installs all required packages.

## Installation

The Makefile automatically sets up a Python virtual environment and installs all dependencies. Simply run:

```bash
make install
```

This will:
1. Create a virtual environment (`venv/`) if it doesn't exist
2. Install/upgrade pip
3. Install all required dependencies from `requirements.txt`

> **Note:** The virtual environment is created automatically and all Makefile commands will use it. You don't need to activate it manually.

### Dependencies

The module requires:
- `langchain-core`: For core functionality
- `langchain-openai`: For OpenAI models and embeddings
- `ragas`: RAG Assessment framework for evaluation metrics
- `langsmith`: LangSmith for evaluation (optional but recommended)
- `python-dotenv`: For environment variable management

All dependencies are listed in `requirements.txt` and installed automatically with `make install`.

### Manual Installation (Alternative)

If you prefer to set up manually:

```bash
# Create virtual environment
python3 -m venv venv

# Activate virtual environment
source venv/bin/activate  # On macOS/Linux
# or
venv\Scripts\activate  # On Windows

# Install dependencies
pip install -r requirements.txt
```

## Running the Examples

### Using Makefile (Recommended)

The Makefile automatically uses the virtual environment:

```bash
# Run the example (creates venv and installs deps if needed)
make run

# Test your knowledge with the quiz
make quiz

# Complete the coding challenge
make challenge
```

### Manual Execution

If you prefer to run manually:

```bash
# Activate virtual environment
source venv/bin/activate  # On macOS/Linux

# Run the example
python rag_evaluation_example.py
```

## Code Examples

### RAG Evaluation Example (`rag_evaluation_example.py`)

This example demonstrates:
- Output analysis using LLMs
- RAGAS faithfulness evaluation
- RAGAS context precision evaluation
- Comparison of different evaluation methods

### Key Components:

1. **Output Analysis**:
   ```python
   from langchain_openai import ChatOpenAI
   from langchain_core.prompts import PromptTemplate

   eval_llm = ChatOpenAI(temperature=0, model="gpt-4o-mini")
   prompt = PromptTemplate(...)
   eval_chain = prompt | eval_llm
   result = eval_chain.invoke({"query": ..., "answer": ..., "result": ...})
   ```

2. **Faithfulness Evaluation**:
   ```python
   from ragas.integrations.langchain import EvaluatorChain
   from ragas.metrics import faithfulness

   faithfulness_chain = EvaluatorChain(
       metric=faithfulness,
       llm=llm,
       embeddings=embeddings
   )
   result = faithfulness_chain({"question": ..., "answer": ..., "contexts": ...})
   ```

3. **Context Precision Evaluation**:
   ```python
   from ragas.metrics import context_precision

   context_precision_chain = EvaluatorChain(
       metric=context_precision,
       llm=llm,
       embeddings=embeddings
   )
   result = context_precision_chain({"question": ..., "ground_truth": ..., "contexts": ...})
   ```

## Common Issues and Solutions

### Issue: Inconsistent Evaluation Results

**Problem:** Evaluation results vary between runs.

**Solution:**
- Set `temperature=0` for evaluation LLMs
- Use the same model version
- Ensure consistent prompt formatting

### Issue: Low Faithfulness Scores

**Problem:** Faithfulness scores are consistently low.

**Solution:**
- Check if retrieved contexts are relevant
- Verify the answer is actually derived from context
- Review for hallucination or misinterpretation

### Issue: Low Context Precision

**Problem:** Context precision scores are low.

**Solution:**
- Improve retrieval quality
- Tune retriever parameters
- Check query understanding
- Consider hybrid retrieval

## Next Steps

After completing this module, you can:
- Set up evaluation pipelines for your RAG applications
- Monitor RAG performance over time
- Optimize retrieval and generation based on metrics
- Build comprehensive evaluation suites

## Quiz

Test your understanding with:

```bash
make quiz
```

## Challenge

Complete the coding challenge:

```bash
make challenge
```

The challenge tests your ability to:
- Create evaluation prompt templates
- Set up RAGAS evaluators
- Use faithfulness metrics
