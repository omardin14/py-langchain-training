"""
Sample Python Code for RAG Application

This module demonstrates a simple RAG (Retrieval-Augmented Generation) application
that can answer questions based on a knowledge base.
"""

import os
from typing import List, Dict
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough


class RAGApplication:
    """
    A simple RAG application that retrieves relevant documents
    and generates answers based on them.
    """
    
    def __init__(self, vector_store: Chroma, model_name: str = "gpt-3.5-turbo"):
        """
        Initialize the RAG application.
        
        Args:
            vector_store: A Chroma vector store containing documents
            model_name: The name of the LLM model to use
        """
        self.vector_store = vector_store
        self.retriever = vector_store.as_retriever()
        self.llm = ChatOpenAI(model=model_name)
        self._setup_chain()
    
    def _setup_chain(self):
        """Set up the retrieval chain."""
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful assistant. Answer questions based on the provided context."),
            ("human", "Context: {context}\n\nQuestion: {question}\n\nAnswer:")
        ])
        
        self.chain = (
            {
                "context": self.retriever,
                "question": RunnablePassthrough()
            }
            | prompt_template
            | self.llm
        )
    
    def query(self, question: str) -> str:
        """
        Query the RAG application with a question.
        
        Args:
            question: The question to ask
            
        Returns:
            The answer generated by the LLM
        """
        response = self.chain.invoke(question)
        return response.content
    
    def add_documents(self, documents: List[Document]):
        """
        Add new documents to the vector store.
        
        Args:
            documents: A list of Document objects to add
        """
        self.vector_store.add_documents(documents)
        # Update retriever after adding documents
        self.retriever = self.vector_store.as_retriever()


def create_vector_store(documents: List[Document], persist_directory: str = "./chroma_db") -> Chroma:
    """
    Create a Chroma vector store from documents.
    
    Args:
        documents: A list of Document objects
        persist_directory: Directory to persist the vector store
        
    Returns:
        A Chroma vector store instance
    """
    embeddings = OpenAIEmbeddings()
    vector_store = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=persist_directory
    )
    return vector_store


def main():
    """Example usage of the RAG application."""
    # Sample documents
    documents = [
        Document(page_content="Python is a programming language.", metadata={"source": "intro"}),
        Document(page_content="Machine learning uses algorithms to learn from data.", metadata={"source": "ml"})
    ]
    
    # Create vector store
    vector_store = create_vector_store(documents)
    
    # Create RAG application
    rag_app = RAGApplication(vector_store)
    
    # Query the application
    question = "What is Python?"
    answer = rag_app.query(question)
    print(f"Question: {question}")
    print(f"Answer: {answer}")


if __name__ == "__main__":
    main()
